{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Zero_Shot_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJp9oPX8MXzYIJ2DSFSd3q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiran74-ds/zsl/blob/main/Zero_Shot_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCaSnVCnyvSp"
      },
      "source": [
        "#### Zero shot learning aims to solve a task to identify classes that are not seen by the model during training.\n",
        "\n",
        "#### For example, letâ€™s say you have seen a horse but never seen a zebra. If I tell you that a zebra looks like a horse but it has black and white stripes, you will probably immediately recognize a zebra when you see it.\n",
        "\n",
        "#### This is what zero-shot learning aims to tackle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UNyYMSNyqjM"
      },
      "source": [
        "#### The high-level strategy we adopt while coding up Zero-shot learning is:\n",
        "1. Import dataset- which constitutes training images and their corresponding class labels\n",
        "2. Fetch the word vectors corresponding to each class from pre-trained word vector models.\n",
        "3. Pass the image through a pre-trained image model like VGG16 and obtain image embedding.\n",
        "4. We expect the network to predict the word vector corresponding to the object in the image.\n",
        "5. Once we train the model, predict word vector on new images.\n",
        "6. The class of word vector that is closest to the predicted word vector is the class of image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dB5QoPs3Mp1"
      },
      "source": [
        "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/06/zero-shot-learning-embedding-based-methods.png\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnTT45vzESbF"
      },
      "source": [
        "**Train Classes**:\n",
        "arm, boy, bread, chicken, child, computer, ear, house, leg, sandwich, television, truck, vehicle, watch, woman\n",
        "\n",
        "**Zero-Shot Classes**:\n",
        "car, food, hand, man, neck\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41fwAHbCyx9p",
        "outputId": "d3632aeb-4424-478b-f818-51fc6d7433a2"
      },
      "source": [
        "!git clone https://github.com/kiran74-ds/zsl.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'zsl'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/31)\u001b[K\rremote: Counting objects:   6% (2/31)\u001b[K\rremote: Counting objects:   9% (3/31)\u001b[K\rremote: Counting objects:  12% (4/31)\u001b[K\rremote: Counting objects:  16% (5/31)\u001b[K\rremote: Counting objects:  19% (6/31)\u001b[K\rremote: Counting objects:  22% (7/31)\u001b[K\rremote: Counting objects:  25% (8/31)\u001b[K\rremote: Counting objects:  29% (9/31)\u001b[K\rremote: Counting objects:  32% (10/31)\u001b[K\rremote: Counting objects:  35% (11/31)\u001b[K\rremote: Counting objects:  38% (12/31)\u001b[K\rremote: Counting objects:  41% (13/31)\u001b[K\rremote: Counting objects:  45% (14/31)\u001b[K\rremote: Counting objects:  48% (15/31)\u001b[K\rremote: Counting objects:  51% (16/31)\u001b[K\rremote: Counting objects:  54% (17/31)\u001b[K\rremote: Counting objects:  58% (18/31)\u001b[K\rremote: Counting objects:  61% (19/31)\u001b[K\rremote: Counting objects:  64% (20/31)\u001b[K\rremote: Counting objects:  67% (21/31)\u001b[K\rremote: Counting objects:  70% (22/31)\u001b[K\rremote: Counting objects:  74% (23/31)\u001b[K\rremote: Counting objects:  77% (24/31)\u001b[K\rremote: Counting objects:  80% (25/31)\u001b[K\rremote: Counting objects:  83% (26/31)\u001b[K\rremote: Counting objects:  87% (27/31)\u001b[K\rremote: Counting objects:  90% (28/31)\u001b[K\rremote: Counting objects:  93% (29/31)\u001b[K\rremote: Counting objects:  96% (30/31)\u001b[K\rremote: Counting objects: 100% (31/31)\u001b[K\rremote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects:   4% (1/23)\u001b[K\rremote: Compressing objects:   8% (2/23)\u001b[K\rremote: Compressing objects:  13% (3/23)\u001b[K\rremote: Compressing objects:  17% (4/23)\u001b[K\rremote: Compressing objects:  21% (5/23)\u001b[K\rremote: Compressing objects:  26% (6/23)\u001b[K\rremote: Compressing objects:  30% (7/23)\u001b[K\rremote: Compressing objects:  34% (8/23)\u001b[K\rremote: Compressing objects:  39% (9/23)\u001b[K\rremote: Compressing objects:  43% (10/23)\u001b[K\rremote: Compressing objects:  47% (11/23)\u001b[K\rremote: Compressing objects:  52% (12/23)\u001b[K\rremote: Compressing objects:  56% (13/23)\u001b[K\rremote: Compressing objects:  60% (14/23)\u001b[K\rremote: Compressing objects:  65% (15/23)\u001b[K\rremote: Compressing objects:  69% (16/23)\u001b[K\rremote: Compressing objects:  73% (17/23)\u001b[K\rremote: Compressing objects:  78% (18/23)\u001b[K\rremote: Compressing objects:  82% (19/23)\u001b[K\rremote: Compressing objects:  86% (20/23)\u001b[K\rremote: Compressing objects:  91% (21/23)\u001b[K\rremote: Compressing objects:  95% (22/23)\u001b[K\rremote: Compressing objects: 100% (23/23)\u001b[K\rremote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 31 (delta 4), reused 27 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (31/31), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZGM7nZ8OeAm"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNQF1nB-KQ4F",
        "outputId": "8cb298ca-1c88-4a56-e7f7-483c3aa6b009"
      },
      "source": [
        "%cd zero-shot-learning/src\n",
        "import gzip\n",
        "import _pickle as cPickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import LabelEncoder, normalize \n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/zero-shot-learning/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXXBJn0-PM2H"
      },
      "source": [
        "#### Image Embeddings\n",
        "\n",
        "Then, image features are extracted from these cropped images using a pre-trained model, VGG16 in our case . Feature extractor class can be seen below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9Yc0ZlzPK7t"
      },
      "source": [
        "DATAPATH = \"../data/zeroshot_data.pkl\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1Jhw5jVPQuN"
      },
      "source": [
        "#### Class Embeddings\n",
        "\n",
        "Class embedding is the vector representation of a class (class label)\n",
        "\n",
        "After we have extracted the image features and formed the datasets, now we should gather the other representations of classes, word embeddings namely. We will use Google Word2Vec representation trained on Google News documents. We will get a Wor2Vec of 300 dimensions for each of the 20 object classes we have specified"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2alIvVRLFEI"
      },
      "source": [
        "WORD2VECPATH = \"../data/class_vectors.npy\" "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnZqvZjjRVKE"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ2pZx7xLYJ7"
      },
      "source": [
        "with open('train_classes.txt', 'r') as infile: \n",
        "  train_classes = [str.strip(line) for line in infile]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZlvnxyhrZyn"
      },
      "source": [
        "with gzip.GzipFile(DATAPATH, 'rb') as infile: \n",
        "    data = cPickle.load(infile)\n",
        "training_data = [instance for instance in data if instance[0] in train_classes]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgdOFmjora93"
      },
      "source": [
        "train_size = 300 # per class\n",
        "train_data, valid_data = [], [] \n",
        "for class_label in train_classes:\n",
        "  ctr = 0\n",
        "  for instance in training_data:\n",
        "    if instance[0] == class_label:\n",
        "      if ctr < train_size:\n",
        "          train_data.append(instance)\n",
        "          ctr+=1 \n",
        "      else:\n",
        "          valid_data.append(instance)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3aVD_HBOnpm"
      },
      "source": [
        "**Load the Data** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVk543MaLb1q"
      },
      "source": [
        "def load_data():\n",
        "  # Load the feature vector data\n",
        "  with gzip.GzipFile(DATAPATH, 'rb') as infile: \n",
        "    data = cPickle.load(infile)\n",
        "\n",
        "  # Define the training classes and the classes that belong to zero-shot. \n",
        "  # Note that, we will only show the classes belonging to training classes and hide the zero-shot model classes until the inference time\n",
        "\n",
        "  training_data = [instance for instance in data if instance[0] in train_classes]\n",
        "  zero_shot_data = [instance for instance in data if instance[0] not in train_classes]\n",
        "  np.random.shuffle(training_data)\n",
        "\n",
        "  # Fetch 300 training images per class for training and the remaining training class images into validation\n",
        "  train_size = 300 # per class\n",
        "  train_data, valid_data = [], [] \n",
        "  for class_label in train_classes:\n",
        "    ctr = 0\n",
        "    for instance in training_data:\n",
        "      if instance[0] == class_label:\n",
        "        if ctr < train_size:\n",
        "            train_data.append(instance)\n",
        "            ctr+=1 \n",
        "        else:\n",
        "            valid_data.append(instance)\n",
        "\n",
        "  # Shuffle the training and validation data and fetch the vectors corresponding to the classes into a dictionary \n",
        "  np.random.shuffle(train_data)\n",
        "  np.random.shuffle(valid_data)\n",
        "  vectors = {i:j for i,j in np.load(WORD2VECPATH, allow_pickle=True)}\n",
        "\n",
        "  # Fetch the training, validation, and zero-shot classes\n",
        "  train_clss = [clss for clss,feat in train_data] \n",
        "  valid_clss = [clss for clss,feat in valid_data] \n",
        "  zero_shot_clss = [clss for clss,feat in zero_shot_data]\n",
        "\n",
        "  # Fetch the image and word embedding features for training and validation data\n",
        "  train_data = [(feat, vectors[clss]) for clss,feat in train_data] \n",
        "  valid_data = [(feat, vectors[clss]) for clss,feat in valid_data]\n",
        "\n",
        "  # Define the input and output arrays of training data, validation data, and zero- shot data\n",
        "  x_train, y_train = zip(*train_data)\n",
        "  x_train, y_train = np.squeeze(np.asarray(x_train)), np.squeeze(np.asarray(y_train))\n",
        "  x_train = normalize(x_train, norm='l2')\n",
        "\n",
        "  x_valid, y_valid = zip(*valid_data)\n",
        "  x_valid, y_valid = np.squeeze(np.asarray(x_valid)), np.squeeze(np.asarray(y_valid))\n",
        "  x_valid = normalize(x_valid, norm='l2')\n",
        "\n",
        "  y_zsl, x_zsl = zip(*zero_shot_data)\n",
        "  x_zsl, y_zsl = np.squeeze(np.asarray(x_zsl)), np.squeeze(np.asarray(y_zsl))\n",
        "  x_zsl = normalize(x_zsl, norm='l2')\n",
        "\n",
        "  # Define the training and validation datasets and dataloaders\n",
        "  trn_ds = TensorDataset(*[torch.Tensor(t).to(device) for t in [x_train, y_train]])\n",
        "  val_ds = TensorDataset(*[torch.Tensor(t).to(device) for t in [x_valid, y_valid]])\n",
        "\n",
        "  trn_dl = DataLoader(trn_ds, batch_size=32, shuffle=True) \n",
        "  val_dl = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
        "\n",
        "  return trn_dl, val_dl, x_zsl, zero_shot_clss, valid_clss, x_valid"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYGAuiW5OArc"
      },
      "source": [
        "**Model Building and Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9J4_ijb03rQ"
      },
      "source": [
        "Build a model that takes the 4096-dimensional feature as input and predicts the 300-dimensional vector as output\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B-Vp8LPMKHZ"
      },
      "source": [
        "def build_model(): \n",
        "  return nn.Sequential(\n",
        "      nn.Linear(4096, 1024), \n",
        "      nn.ReLU(inplace=True), \n",
        "      nn.BatchNorm1d(1024), \n",
        "      nn.Dropout(0.8), \n",
        "      nn.Linear(1024, 512), \n",
        "      nn.ReLU(inplace=True), \n",
        "      nn.BatchNorm1d(512), \n",
        "      nn.Dropout(0.8), \n",
        "      nn.Linear(512, 256), \n",
        "      nn.ReLU(inplace=True), \n",
        "      nn.BatchNorm1d(256), \n",
        "      nn.Dropout(0.8), \n",
        "      nn.Linear(256, 300)\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j3S8pkP08qt"
      },
      "source": [
        "Functions to train and validate on a batch of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F54GYaMxMYhe"
      },
      "source": [
        "def train_batch(model, data, optimizer, criterion):\n",
        "  model.train()\n",
        "  ims, labels = data\n",
        "  _preds = model(ims)\n",
        "  optimizer.zero_grad()\n",
        "  loss = criterion(_preds, labels)\n",
        "  loss.backward() \n",
        "  optimizer.step() \n",
        "  return loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_batch(model, data, criterion):\n",
        "  model.eval()\n",
        "  ims, labels = data\n",
        "  _preds = model(ims)\n",
        "  loss = criterion(_preds, labels) \n",
        "  \n",
        "  return loss.item()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8aZfhaL1FMp"
      },
      "source": [
        "Train the model over increasing epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAGyL6aAGO3w",
        "outputId": "2cdf72ba-7f3c-48d4-dde2-1dce603861e3"
      },
      "source": [
        "trn_dl, val_dl, x_zsl, zero_shot_clss, valid_clss, x_valid = load_data()\n",
        "model = build_model().to(device)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (3): Dropout(p=0.8, inplace=False)\n",
            "  (4): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (7): Dropout(p=0.8, inplace=False)\n",
            "  (8): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (9): ReLU(inplace=True)\n",
            "  (10): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (11): Dropout(p=0.8, inplace=False)\n",
            "  (12): Linear(in_features=256, out_features=300, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_LAsJygMnoi",
        "outputId": "61cf6cd3-92b7-412d-971e-d14160588c35"
      },
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3) \n",
        "\n",
        "n_epochs = 65\n",
        "train_losses, valid_losses = [], []\n",
        "#log = Report(n_epochs) \n",
        "for ex in range(n_epochs):\n",
        "  N = len(trn_dl)\n",
        "  running_loss = 0 \n",
        "  for bx, data in enumerate(trn_dl):\n",
        "    batch_loss = train_batch(model, data, optimizer, criterion) \n",
        "    running_loss += batch_loss *32 \n",
        "  train_epoch_loss = running_loss / N\n",
        "  train_losses.append(train_epoch_loss)\n",
        "    #log.record(ex+(bx+1)/N, trn_loss=loss, end='\\r')\n",
        "\n",
        "  N = len(val_dl)\n",
        "  running_loss = 0 \n",
        "  for bx, data in enumerate(val_dl):\n",
        "    batch_loss = validate_batch(model, data, criterion)\n",
        "    running_loss += batch_loss *32 \n",
        "  valid_epoch_loss = running_loss / N\n",
        "  valid_losses.append(valid_epoch_loss)\n",
        "    #log.record(ex+(bx+1)/N, val_loss=loss, end='\\r')\n",
        "  print(\"Epoch {} , training loss {}, valid loss {}\".format(ex+1, train_epoch_loss, valid_epoch_loss))\n",
        "\n",
        "  if ex == 10: optimizer = optim.Adam(model.parameters(), lr=1e-4) \n",
        "  if ex == 40: optimizer = optim.Adam(model.parameters(), lr=1e-5) \n",
        "  #if not (ex+1)%10: log.report_avgs(ex+1)\n",
        "\n",
        "#log.plot(log=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 , training loss 19.120066272451524, valid loss 0.6701433772736407\n",
            "Epoch 2 , training loss 1.3871672115427383, valid loss 0.5630186542551568\n",
            "Epoch 3 , training loss 0.6363185893559287, valid loss 0.5490318701622334\n",
            "Epoch 4 , training loss 0.6239724666514295, valid loss 0.5400513879796291\n",
            "Epoch 5 , training loss 0.6247138905187025, valid loss 0.5322176503374222\n",
            "Epoch 6 , training loss 0.622259498910701, valid loss 0.5161788577729083\n",
            "Epoch 7 , training loss 0.6281832029633488, valid loss 0.5060342004958619\n",
            "Epoch 8 , training loss 0.6227631856363716, valid loss 0.49523815132201987\n",
            "Epoch 9 , training loss 0.625836211316129, valid loss 0.48405677650837187\n",
            "Epoch 10 , training loss 0.6208950310734147, valid loss 0.4713025391101837\n",
            "Epoch 11 , training loss 0.6144149705027857, valid loss 0.4671480490806255\n",
            "Epoch 12 , training loss 0.5516382899690182, valid loss 0.44880952353173115\n",
            "Epoch 13 , training loss 0.505748468933376, valid loss 0.439536507459397\n",
            "Epoch 14 , training loss 0.4949049989805154, valid loss 0.43344173659669594\n",
            "Epoch 15 , training loss 0.48332577077209526, valid loss 0.42858542850677\n",
            "Epoch 16 , training loss 0.4824357231458028, valid loss 0.41795146084846335\n",
            "Epoch 17 , training loss 0.4773815337647783, valid loss 0.411989226620248\n",
            "Epoch 18 , training loss 0.4703060950793273, valid loss 0.4059046614677348\n",
            "Epoch 19 , training loss 0.461424283736141, valid loss 0.3992416433831479\n",
            "Epoch 20 , training loss 0.4571355883534073, valid loss 0.39065548206897494\n",
            "Epoch 21 , training loss 0.4503978173783485, valid loss 0.3828535428706636\n",
            "Epoch 22 , training loss 0.4463142957670469, valid loss 0.3763521795577191\n",
            "Epoch 23 , training loss 0.4415283302466075, valid loss 0.3776526685724867\n",
            "Epoch 24 , training loss 0.43352746794409786, valid loss 0.37113927590086104\n",
            "Epoch 25 , training loss 0.4259298061225431, valid loss 0.3669253321404153\n",
            "Epoch 26 , training loss 0.423453022613593, valid loss 0.36299872588604054\n",
            "Epoch 27 , training loss 0.42155432616565247, valid loss 0.35689356542648154\n",
            "Epoch 28 , training loss 0.41466603528523277, valid loss 0.3569346457085711\n",
            "Epoch 29 , training loss 0.41510761291422743, valid loss 0.3585142518611664\n",
            "Epoch 30 , training loss 0.4089073828348877, valid loss 0.353593203615635\n",
            "Epoch 31 , training loss 0.40755909554501796, valid loss 0.35042705117387973\n",
            "Epoch 32 , training loss 0.4018095275189014, valid loss 0.348277655687738\n",
            "Epoch 33 , training loss 0.40099818972831075, valid loss 0.34832297550871016\n",
            "Epoch 34 , training loss 0.3945871908613976, valid loss 0.34557009060332117\n",
            "Epoch 35 , training loss 0.3879527470744248, valid loss 0.3436201772791274\n",
            "Epoch 36 , training loss 0.38596612658906493, valid loss 0.34171862107642154\n",
            "Epoch 37 , training loss 0.3855348690181759, valid loss 0.33905913855167147\n",
            "Epoch 38 , training loss 0.3793004419363982, valid loss 0.33783614952513513\n",
            "Epoch 39 , training loss 0.3800726680890888, valid loss 0.33554718278824014\n",
            "Epoch 40 , training loss 0.3791680139430026, valid loss 0.33401779545114396\n",
            "Epoch 41 , training loss 0.37837272924734344, valid loss 0.33223352660524086\n",
            "Epoch 42 , training loss 0.3687695397975597, valid loss 0.3314208185419123\n",
            "Epoch 43 , training loss 0.3663180675489683, valid loss 0.332540049197826\n",
            "Epoch 44 , training loss 0.36369792197613005, valid loss 0.33028601078276937\n",
            "Epoch 45 , training loss 0.36148204490648095, valid loss 0.3314503532774905\n",
            "Epoch 46 , training loss 0.3612301009343871, valid loss 0.33005523491413036\n",
            "Epoch 47 , training loss 0.3587728134283783, valid loss 0.33134729684667386\n",
            "Epoch 48 , training loss 0.3589870502762761, valid loss 0.3305915055122781\n",
            "Epoch 49 , training loss 0.35630033603796724, valid loss 0.32791765319540145\n",
            "Epoch 50 , training loss 0.35511330700089744, valid loss 0.32800227593868336\n",
            "Epoch 51 , training loss 0.3521475667226399, valid loss 0.3292211871197883\n",
            "Epoch 52 , training loss 0.3556970928577667, valid loss 0.3306574009834452\n",
            "Epoch 53 , training loss 0.35592696937263435, valid loss 0.3299682273509655\n",
            "Epoch 54 , training loss 0.35310455522638684, valid loss 0.3282675476784402\n",
            "Epoch 55 , training loss 0.35264730390082016, valid loss 0.3260470647761162\n",
            "Epoch 56 , training loss 0.3498016380249186, valid loss 0.32786848887484127\n",
            "Epoch 57 , training loss 0.3534655342710779, valid loss 0.3267267625382606\n",
            "Epoch 58 , training loss 0.3504882440076652, valid loss 0.3256780143747938\n",
            "Epoch 59 , training loss 0.3507686012180139, valid loss 0.3297404386895768\n",
            "Epoch 60 , training loss 0.34957525810451373, valid loss 0.32709060514226873\n",
            "Epoch 61 , training loss 0.3503578336949044, valid loss 0.3241228829038904\n",
            "Epoch 62 , training loss 0.34755388007941823, valid loss 0.3269104729307459\n",
            "Epoch 63 , training loss 0.34963957384123023, valid loss 0.3254892572443536\n",
            "Epoch 64 , training loss 0.3464119170151704, valid loss 0.3271877454950454\n",
            "Epoch 65 , training loss 0.3486054584066919, valid loss 0.3249420238302109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "Qu1A5F4Ftd4S",
        "outputId": "7a65e2a0-912d-4afc-9e77-f2796160ebaa"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "epochs = np.arange(65)+1\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.subplot(121)\n",
        "plt.title('Loss value over increasing epochs') \n",
        "plt.plot(epochs, train_losses, label='Training Loss') \n",
        "plt.plot(epochs, valid_losses, label='Validation Loss', linestyle='dashed') \n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.ylim(0, 2.5)\n",
        "plt.legend();"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lvF9aYEN0lj"
      },
      "source": [
        "**Model Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4flzHpt1MHd"
      },
      "source": [
        "Predict on images that contain the zero-shot classes (classes which the model has not seen) and also fetch the actual features and classnames corresponding to the zero-shot classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJmSxApop8ZE",
        "outputId": "df33fd60-f1ab-47ad-ae2f-36a95f7d09d6"
      },
      "source": [
        "pred_zsl = model(torch.Tensor(x_valid).to(device)).cpu().detach().numpy()\n",
        "\n",
        "class_vectors = sorted(np.load(WORD2VECPATH, allow_pickle=True), key=lambda x: x[0])\n",
        "classnames, vectors = zip(*class_vectors)\n",
        "classnames = list(classnames)\n",
        "\n",
        "vectors = np.array(vectors)\n",
        "\n",
        "dists = (pred_zsl[None] - vectors[:,None]) \n",
        "dists = (dists**2).sum(-1).T\n",
        "\n",
        "best_classes = []\n",
        "for item in dists:\n",
        "  best_classes.append([classnames[j] for j in np.argsort(item)[:5]])\n",
        "\n",
        "np.mean([i in J for i,J in zip(valid_clss, best_classes)])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8814467515070328"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JRcKEOFNDnu"
      },
      "source": [
        "pred_zsl = model(torch.Tensor(x_zsl).to(device)).cpu().detach().numpy()\n",
        "\n",
        "class_vectors = sorted(np.load(WORD2VECPATH, allow_pickle=True), key=lambda x: x[0])\n",
        "classnames, vectors = zip(*class_vectors)\n",
        "classnames = list(classnames)\n",
        "\n",
        "vectors = np.array(vectors)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVCqiKhl1PNZ"
      },
      "source": [
        "Calculate the distance between each predicted vector and the vector corresponding to the available classes and measure the number of zero-shot classes present in the top 5 predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOV51JrW1PgS",
        "outputId": "01880cbb-4803-4f19-bbc1-07ced685411d"
      },
      "source": [
        "dists = (pred_zsl[None] - vectors[:,None]) \n",
        "dists = (dists**2).sum(-1).T\n",
        "\n",
        "best_classes = []\n",
        "for item in dists:\n",
        "  best_classes.append([classnames[j] for j in np.argsort(item)[:5]])\n",
        "\n",
        "np.mean([i in J for i,J in zip(zero_shot_clss, best_classes)])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7978989494747374"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2Onv0EYyckM"
      },
      "source": [
        "We can predict correctly for ~80% of the images that contain an object whose class is not present during training, in the top 5 predictions of the model.\n",
        "Note that the percentage of correctly classified images will be 6%, 14%, and 40% for top 1,2,3 predictions, respectively"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0bJoy84fFdz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}