{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem Description\n",
    "\n",
    "In a conventional object recognition process, It is necessary for a machine learning model to be trained on many samples to be able to recognize an object. But, we want our model to recognize an object it did not see during the training phase.\n",
    "\n",
    "**Zero-Shot learning method aims to solve a task without receiving any example of that task at training phase**\n",
    "\n",
    "For example, letâ€™s say you have seen a horse but never seen a zebra. If I tell you that a zebra looks like a horse but it has black and white stripes, you will probably immediately recognize a zebra when you see it. This is what zero-shot learning aims to tackle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://www.programmersought.com/images/779/99f80acb97763b3b810b016933de6d73.png\" width=600, height=400 align=\"right\">\n",
    "In zero shot learning the data consists of\n",
    "\n",
    "+ **Seen classes**: These are classes for which we have labelled images during training. **Example horse, donkey, tiger, hyena, panda, penguin as shown in image**\n",
    "\n",
    "+ **Unseen classes**: These are classes for which labelled images are not present during the training phase. **Here Zebra is an unseen class**.\n",
    "\n",
    "+ **Auxiliary information**: This information consists of descriptions for both seen and unseen classes at train time. This information acts as a bridge between seen and unseen classes.\n",
    "\n",
    "+ **As shown in the picture, using zero shot learning, the model is now able to identify zebra even though it has not seen zebra during the training.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "The auxillary information which is nothing but a text description of seen and unseen classes is converted to a numerical represenation called **word vector or class embedding**.\n",
    "\n",
    "Now let us consider an example of a cat which has a description \"cat has a tail, fur and whiskers\".\n",
    "The corresponding word vector for this cat can be determined as shown in the following image.\n",
    "\n",
    "<img\n",
    "     src=\"https://www.learnopencv.com/wp-content/uploads/2020/06/attribute-vector-in-zero-shot-learning.png\"\n",
    "     width=300, height=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img align=\"right\" src=\"https://github.com/kiran74-ds/zsl/blob/main/data/Images/word2vec.png?raw=true\" width=450, height=300>\n",
    "It is safe to then say that all animals would have similar word vectors and all automobiles would have similar word vectors but the word vector representations of these two categories would vary from each other.\n",
    "\n",
    "From the picture, we can see that the word vectors of automobiles fall to the left of the chart while the vectors corresponding to animals are on the right. Further, similar animals also have similar vectors.\n",
    "This gives us the intuition that words, just like images, also have vector embeddings that help in obtaining similarity.\n",
    "\n",
    "We will leverage this phenomenon to identify classes that are not seen by the model during training - Essentially, we will learn about mapping image features to word features, directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The high-level strategy we adopt to solve Zero-shot learning is:\n",
    "1. Import dataset- which constitutes training images and their corresponding classes.\n",
    "2. Fetch the **word vectors(class embedding)** corresponding to each class from pre-trained word vector models.\n",
    "3. Pass the image through a pre-trained image model like VGG16 and obtain **image embedding**.\n",
    "4. We expect the network to predict the **word vector(class embedding)** given an image.\n",
    "5. Once we train the model, predict **word vector(class embedding)** on new images.\n",
    "6. The class of word vector that is closest to the predicted word vector is the class of image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training Architecture\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/06/zero-shot-learning-embedding-based-methods.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wrap Up\n",
    "\n",
    "+ After implementing on Zero shot learning on Imagenet dataset with 15 seen classes and 5 unseen class, I found that  the model is able to correctly predict ~78% on unseen classes, in the top 5 predictions of the model.Note that the percentage of correctly classified images will be 6%, 14%, and 40% for top 1,2,3 predictions, respectively\n",
    "\n",
    "+ With the developments in robotics field, we are trying to produce robots that are similar to humans. Human vision is among the most important characteristics that makes us human and we want to transfer this feature to robots. We are able to interpret and recognize an object even if we have never seen a sample where at least we can reason about what that thing is. \n",
    "\n",
    "+ **Zero-Shot learning** method is similar to human vision system in many ways, therefore it can be used in robot vision. Instead of performing recognition on a limited set of objects, using Zero-Shot learning, it is possible to recognize every world object.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
